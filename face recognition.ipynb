{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import mediapipe as mp\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import itertools\n",
    "from model import KeyPointClassifier\n",
    "from pprint import pprint\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Open', 'Close', 'Pointer', 'OK']\n"
     ]
    }
   ],
   "source": [
    " with open('model/keypoint_classifier/keypoint_classifier_label.csv',\n",
    "              encoding='utf-8-sig') as f:\n",
    "        keypoint_classifier_labels = csv.reader(f)\n",
    "        keypoint_classifier_labels = [\n",
    "            row[0] for row in keypoint_classifier_labels\n",
    "        ]\n",
    "print(keypoint_classifier_labels)\n",
    "keypoint_classifier = KeyPointClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        # landmark_z = landmark.z\n",
    "\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_landmark_list = list(\n",
    "        itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "    return temp_landmark_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewant_image = face_recognition.load_image_file(\"rewant.jpg\")\n",
    "\n",
    "rewant_face_encoding = face_recognition.face_encodings(rewant_image)[0]\n",
    "\n",
    "sanjay_image = face_recognition.load_image_file(\"sanjay.jpg\")\n",
    "\n",
    "sanjay_face_encoding = face_recognition.face_encodings(sanjay_image)[0]\n",
    "\n",
    "samikshya_image = face_recognition.load_image_file(\"samikshya.jpg\")\n",
    "\n",
    "samikshya_face_encoding = face_recognition.face_encodings(samikshya_image)[0]\n",
    "# print(rewant_face_encoding)\n",
    "\n",
    "known_face_encodings = [\n",
    "    rewant_face_encoding,\n",
    "    samikshya_face_encoding,\n",
    "    sanjay_face_encoding\n",
    "]\n",
    "\n",
    "known_face_names = [\n",
    "    \"Rewant\",\n",
    "    \"Samikshya\",\n",
    "    \"Sanjay\"\n",
    "]\n",
    "\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open\n",
      "Open\n",
      "Open\n",
      "Open\n",
      "Open\n",
      "Open\n",
      "Open\n",
      "Open\n",
      "Open\n",
      "Open\n",
      "Open\n",
      "Open\n",
      "Close\n",
      "Close\n",
      "Close\n",
      "Close\n",
      "Close\n",
      "Close\n",
      "Close\n",
      "Close\n",
      "Close\n",
      "Close\n",
      "Close\n",
      "Close\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Open\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Open\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "Pointer\n",
      "['Rewant']\n"
     ]
    }
   ],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_face_detection.FaceDetection(\n",
    "    min_detection_confidence=0.5) as face_detection, mp_hands.Hands(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5, max_num_hands=10) as hands, mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as holistic:\n",
    " while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "#     print(image.shape[1])\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # Flip the image horizontally for a later selfie-view display, and convert\n",
    "    # the BGR image to RGB.\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    \n",
    "    if process_this_frame:\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(image)\n",
    "#         print(face_locations[0])\n",
    "        face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "#         print(face_landmarks_list)\n",
    "        if(face_landmarks_list!=[]):\n",
    "#             print(face_landmarks_list[0]['nose_tip'][0][0])\n",
    "#             print(face_landmarks_list[0]['nose_tip'][0][1])\n",
    "            nose_x = face_landmarks_list[0]['nose_tip'][0][0]\n",
    "            nose_y = face_landmarks_list[0]['nose_tip'][0][1]\n",
    "        face_encodings = face_recognition.face_encodings(image, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # # If a match was found in known_face_encodings, just use the first one.\n",
    "            # if True in matches:\n",
    "            #     first_match_index = matches.index(True)\n",
    "            #     name = known_face_names[first_match_index]\n",
    "\n",
    "            # Or instead, use the known face with the smallest distance to the new face\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "    \n",
    "    \n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    face_results = face_detection.process(image)\n",
    "    hand_results = hands.process(image)\n",
    "#     holistic_results = holistic.process(image)\n",
    "\n",
    "    # Draw the face detection annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     mp_drawing.draw_landmarks(\n",
    "#         image, holistic_results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "#     mp_drawing.draw_landmarks(\n",
    "#         image, holistic_results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "#     mp_drawing.draw_landmarks(\n",
    "#         image, holistic_results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "#     print(holistic_results.left_hand_landmarks)\n",
    "#     print(holistic_results.right_hand_landmarks)\n",
    "#     if hand_results.multi_hand_landmarks:\n",
    "#         print((hand_results.multi_hand_landmarks[0]))\n",
    "#     print(vars(hand_results))\n",
    "#     print(face_results)\n",
    "    \n",
    "    if face_results.detections:\n",
    "      for detection in face_results.detections:\n",
    "#         print(detection.location_data.relative_bounding_box)\n",
    "#         print('Nose tip:')\n",
    "#         print(mp_face_detection.get_key_point(\n",
    "#           detection, mp_face_detection.FaceKeyPoint.LEFT_EYE))\n",
    "        mp_drawing.draw_detection(image, detection)\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "      for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "        hand_x = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP].x * image.shape[1]\n",
    "        hand_y = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP].y * image.shape[0]\n",
    "        dist = math.sqrt(pow((hand_x-nose_x),2)+pow((hand_y-nose_y),2))\n",
    "#         print('distance is')\n",
    "#         print(dist)\n",
    "#         print(hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP].y * image.shape[0])\n",
    "#         print(hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP].x * image.shape[1])\n",
    "#         print(hand_landmarks)\n",
    "        \n",
    "        landmark_list = calc_landmark_list(image, hand_landmarks)\n",
    "\n",
    "        # Conversion to relative coordinates / normalized coordinates\n",
    "        pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
    "        hand_sign_id = keypoint_classifier(pre_processed_landmark_list)\n",
    "        print(keypoint_classifier_labels[hand_sign_id])\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    cv2.imshow('Face and Hand Detection', image)\n",
    "#     print(cv2.getWindowImageRect('Face and Hand Detection'))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "# print(mp_holistic.HAND_CONNECTIONS)\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "print(face_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
