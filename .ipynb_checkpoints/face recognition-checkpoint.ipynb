{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import mediapipe as mp\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import copy\n",
    "import itertools\n",
    "from model import KeyPointClassifier\n",
    "from pprint import pprint\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Open', 'Close', 'Pointer', 'OK']\n"
     ]
    }
   ],
   "source": [
    " with open('model/keypoint_classifier/keypoint_classifier_label.csv',\n",
    "              encoding='utf-8-sig') as f:\n",
    "        keypoint_classifier_labels = csv.reader(f)\n",
    "        keypoint_classifier_labels = [\n",
    "            row[0] for row in keypoint_classifier_labels\n",
    "        ]\n",
    "print(keypoint_classifier_labels)\n",
    "keypoint_classifier = KeyPointClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        # landmark_z = landmark.z\n",
    "\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_landmark_list = list(\n",
    "        itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "    return temp_landmark_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.170957    0.02128921  0.08682538 -0.00960241  0.01238865 -0.10986894\n",
      " -0.02933855 -0.04556401  0.12331875 -0.07569244  0.23622094  0.03177498\n",
      " -0.18906187 -0.10346113  0.03745404  0.08722275 -0.20988569 -0.05905652\n",
      " -0.00752922 -0.06977211  0.13798246  0.02738542  0.08508907  0.04563217\n",
      " -0.1698968  -0.35679704 -0.15631381 -0.14264761  0.01614265 -0.01641001\n",
      " -0.02180443  0.04810834 -0.20803502 -0.02233122  0.02098369  0.07013591\n",
      "  0.05194379 -0.01305919  0.13351104 -0.00303699 -0.12550145 -0.04315693\n",
      "  0.00612212  0.22655542  0.11918036  0.12946619  0.03339425 -0.02653782\n",
      "  0.11681515 -0.17631952  0.08842823  0.12752904  0.07602984  0.00150159\n",
      "  0.11201639 -0.09908703  0.05966721  0.03501166 -0.19720607  0.04147689\n",
      "  0.0159059  -0.08469264  0.02387163 -0.00981193  0.16061905  0.07220344\n",
      " -0.08287104 -0.08058067  0.14788103 -0.13274184  0.00345007  0.08470626\n",
      " -0.13027814 -0.21036717 -0.24258275  0.03121726  0.44321179  0.13576673\n",
      " -0.10568318  0.0917405  -0.09477936 -0.07552402  0.12572572  0.07185411\n",
      " -0.06093214  0.07359821 -0.14654604  0.11111718  0.19862764 -0.03333748\n",
      " -0.01931312  0.22254458 -0.03514906  0.07282194  0.02808046 -0.00943355\n",
      " -0.06692582  0.02941448 -0.07029191  0.01957416  0.05450946 -0.04294757\n",
      " -0.03234467  0.03332896 -0.13518915  0.03185368  0.00967114 -0.04549885\n",
      " -0.07056507  0.01345281 -0.1759728  -0.05554183  0.03361186 -0.26312771\n",
      "  0.1778942   0.16543528 -0.02954948  0.17477132  0.00519029  0.06142979\n",
      " -0.00636189 -0.09947659 -0.14279932 -0.07285452  0.11726765  0.03270287\n",
      "  0.09165794 -0.01745724]\n"
     ]
    }
   ],
   "source": [
    "rewant_image = face_recognition.load_image_file(\"rewant.jpg\")\n",
    "\n",
    "rewant_face_encoding = face_recognition.face_encodings(rewant_image)[0]\n",
    "\n",
    "samikshya_image = face_recognition.load_image_file(\"samikshya.jpg\")\n",
    "\n",
    "samikshya_face_encoding = face_recognition.face_encodings(samikshya_image)[0]\n",
    "print(rewant_face_encoding)\n",
    "\n",
    "known_face_encodings = [\n",
    "    rewant_face_encoding,\n",
    "    samikshya_face_encoding\n",
    "]\n",
    "\n",
    "known_face_names = [\n",
    "    \"Rewant\",\n",
    "    \"Samikshya\"\n",
    "]\n",
    "\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(219, 356, 374, 201)]\n",
      "[(219, 356, 374, 201)]\n",
      "[(219, 356, 374, 201)]\n",
      "[(219, 356, 374, 201)]\n",
      "[(236, 408, 391, 253)]\n",
      "[(201, 448, 387, 262)]\n",
      "[(222, 469, 407, 283)]\n",
      "[(222, 448, 407, 262)]\n",
      "[(222, 428, 407, 242)]\n",
      "[(211, 368, 340, 239)]\n",
      "['Rewant']\n"
     ]
    }
   ],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_face_detection.FaceDetection(\n",
    "    min_detection_confidence=0.5) as face_detection, mp_hands.Hands(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5, max_num_hands=10) as hands, mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as holistic:\n",
    " while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # Flip the image horizontally for a later selfie-view display, and convert\n",
    "    # the BGR image to RGB.\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    \n",
    "    if process_this_frame:\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(image)\n",
    "#         print(face_locations)\n",
    "        face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "#         print(face_landmarks_list)\n",
    "        face_encodings = face_recognition.face_encodings(image, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # # If a match was found in known_face_encodings, just use the first one.\n",
    "            # if True in matches:\n",
    "            #     first_match_index = matches.index(True)\n",
    "            #     name = known_face_names[first_match_index]\n",
    "\n",
    "            # Or instead, use the known face with the smallest distance to the new face\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "    \n",
    "    \n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    face_results = face_detection.process(image)\n",
    "    hand_results = hands.process(image)\n",
    "#     holistic_results = holistic.process(image)\n",
    "\n",
    "    # Draw the face detection annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     mp_drawing.draw_landmarks(\n",
    "#         image, holistic_results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "#     mp_drawing.draw_landmarks(\n",
    "#         image, holistic_results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "#     mp_drawing.draw_landmarks(\n",
    "#         image, holistic_results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "#     print(holistic_results.left_hand_landmarks)\n",
    "#     print(holistic_results.right_hand_landmarks)\n",
    "#     if hand_results.multi_hand_landmarks:\n",
    "#         print((hand_results.multi_hand_landmarks[0]))\n",
    "#     print(vars(hand_results))\n",
    "#     print(face_results)\n",
    "    \n",
    "    if face_results.detections:\n",
    "      for detection in face_results.detections:\n",
    "#         print(detection.location_data.relative_bounding_box)\n",
    "#         print('Nose tip:')\n",
    "#         print(mp_face_detection.get_key_point(\n",
    "#           detection, mp_face_detection.FaceKeyPoint.LEFT_EYE))\n",
    "        mp_drawing.draw_detection(image, detection)\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "      for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "        print(hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP])\n",
    "#         print(hand_landmarks)\n",
    "        landmark_list = calc_landmark_list(image, hand_landmarks)\n",
    "\n",
    "        # Conversion to relative coordinates / normalized coordinates\n",
    "        pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
    "        hand_sign_id = keypoint_classifier(pre_processed_landmark_list)\n",
    "        print(keypoint_classifier_labels[hand_sign_id])\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    cv2.imshow('Face and Hand Detection', image)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "# print(mp_holistic.HAND_CONNECTIONS)\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "print(face_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
